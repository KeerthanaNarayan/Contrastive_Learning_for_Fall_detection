{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KeerthanaNarayan/Contrastive_Learning_for_Fall_detection/blob/main/SimCLR_Training_Phase_Week_6.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AWnl1R-kTuqd",
        "outputId": "267d48e7-4eda-4bbc-edc9-958ee2cd0189"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/10], Loss: 3.242416078900435\n",
            "Epoch [2/10], Loss: 3.22059810826573\n",
            "Epoch [3/10], Loss: 3.20605821241998\n",
            "Epoch [4/10], Loss: 3.2142134552177897\n",
            "Epoch [5/10], Loss: 3.2006392361755283\n",
            "Epoch [6/10], Loss: 3.2003081892176124\n",
            "Epoch [7/10], Loss: 3.20280687717565\n",
            "Epoch [8/10], Loss: 3.1952038601111146\n",
            "Epoch [9/10], Loss: 3.19566827932253\n",
            "Epoch [10/10], Loss: 3.21330200153859\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import zipfile\n",
        "import glob\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# Load and preprocess the accelerometer data\n",
        "def load_and_preprocess_data():\n",
        "    zip_path = 'fall-dataset-all.zip' #zip_path = 'fall-dataset-all.zip'\n",
        "\n",
        "    # Extract the CSV files from the zip file\n",
        "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "        csv_files = [file for file in zip_ref.namelist() if file.endswith('.csv')]\n",
        "        zip_ref.extractall(members=csv_files)\n",
        "\n",
        "    # Read and concatenate the extracted CSV files into a DataFrame\n",
        "    data = pd.concat([pd.read_csv(file, encoding='latin-1') for file in csv_files], ignore_index=True)\n",
        "    accelerometer_data = data[[\"Acc(X)\", \"Acc(Y)\", \"Acc(Z)\", \"Rot(X)\", \"Rot(Y)\", \"Rot(Z)\", \"Pitch\", \"Roll\", \"Yaw\", \"Timestamp\"]].values\n",
        "\n",
        "    # Standardize the data\n",
        "    scaler = StandardScaler()\n",
        "    standardized_data = scaler.fit_transform(accelerometer_data)\n",
        "\n",
        "    return standardized_data\n",
        "\n",
        "# Define the augmentation function\n",
        "def augment_function(sample):\n",
        "    augmented_sample = apply_augmentation(sample)\n",
        "    return augmented_sample\n",
        "\n",
        "# Define augmentation functions\n",
        "def apply_augmentation(sample):\n",
        "    augmented_sample = sample.copy()\n",
        "\n",
        "    # Noise Injection\n",
        "    noise = np.random.normal(loc=0, scale=0.1, size=augmented_sample.shape)\n",
        "    augmented_sample += noise\n",
        "\n",
        "    # Time Shifting\n",
        "    shift_amount = np.random.randint(low=1, high=len(augmented_sample))\n",
        "    augmented_sample = np.roll(augmented_sample, shift_amount, axis=0)\n",
        "\n",
        "    # Magnitude Scaling\n",
        "    scaling_factor = np.random.uniform(low=0.8, high=1.2)\n",
        "    augmented_sample *= scaling_factor\n",
        "\n",
        "    return augmented_sample\n",
        "\n",
        "# Define dataset class for accelerometer data\n",
        "class AccelerometerDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, data):\n",
        "        self.data = data\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        sample = self.data[index]\n",
        "        augmented_sample_1 = augment_function(sample)\n",
        "        augmented_sample_2 = augment_function(sample)\n",
        "        return augmented_sample_1, augmented_sample_2\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "# Load and preprocess your accelerometer data\n",
        "data = load_and_preprocess_data()\n",
        "\n",
        "# Create the dataset\n",
        "dataset = AccelerometerDataset(data)\n",
        "\n",
        "# Create the data loader\n",
        "batch_size = 64\n",
        "data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "# Define the SimCLR model architecture\n",
        "class SimCLRModel(nn.Module):\n",
        "    def __init__(self, num_steps):\n",
        "        super(SimCLRModel, self).__init__()\n",
        "        self.embedding_size = 128\n",
        "        self.num_steps = num_steps\n",
        "\n",
        "        self.backbone = nn.Sequential(\n",
        "            nn.Conv1d(1, 64, kernel_size=3, stride=1, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool1d(kernel_size=2, stride=2),\n",
        "            nn.Conv1d(64, 128, kernel_size=3, stride=1, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool1d(kernel_size=2, stride=2)\n",
        "        )\n",
        "\n",
        "        self.fc = nn.Linear(128 * (num_steps // 4), self.embedding_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.backbone(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        embedding = self.fc(x)\n",
        "        return embedding\n",
        "\n",
        "# Define the contrastive loss function\n",
        "class ContrastiveLoss(nn.Module):\n",
        "    def __init__(self, temperature=1.0):\n",
        "        super(ContrastiveLoss, self).__init__()\n",
        "        self.temperature = temperature\n",
        "\n",
        "    def forward(self, embeddings_1, embeddings_2):\n",
        "        # Normalize the embeddings\n",
        "        embeddings_1 = nn.functional.normalize(embeddings_1, dim=1)\n",
        "        embeddings_2 = nn.functional.normalize(embeddings_2, dim=1)\n",
        "\n",
        "        # Calculate cosine similarity between the embeddings\n",
        "        similarities = torch.matmul(embeddings_1, embeddings_2.T) / self.temperature\n",
        "\n",
        "        # Generate target labels (1 for positive pairs, 0 for negative pairs)\n",
        "        labels = torch.arange(embeddings_1.size(0)).to(embeddings_1.device)\n",
        "\n",
        "        # Calculate contrastive loss\n",
        "        loss = nn.CrossEntropyLoss()(similarities, labels)\n",
        "\n",
        "        return loss\n",
        "\n",
        "# Initialize the SimCLR model, contrastive loss, and optimizer\n",
        "num_steps = dataset[0][0].shape[0]\n",
        "model = SimCLRModel(num_steps).double()\n",
        "criterion = ContrastiveLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Training loop\n",
        "num_epochs = 10\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    for batch in data_loader:\n",
        "        # Clear the gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Get the batch of augmented samples\n",
        "        augmented_samples_1, augmented_samples_2 = batch\n",
        "\n",
        "        # Reshape the input data to include the num_channels dimension\n",
        "        augmented_samples_1 = augmented_samples_1.unsqueeze(1)\n",
        "        augmented_samples_2 = augmented_samples_2.unsqueeze(1)\n",
        "\n",
        "        # Forward pass\n",
        "        embeddings_1 = model(augmented_samples_1.to(device).double())\n",
        "        embeddings_2 = model(augmented_samples_2.to(device).double())\n",
        "\n",
        "        # Calculate the contrastive loss\n",
        "        loss = criterion(embeddings_1, embeddings_2)\n",
        "\n",
        "        # Backward pass\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item()}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EZjRiDmJ30aj",
        "outputId": "6e1a5c5f-dd04-41d4-ea43-8689952f7168",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "SimCLRModel(\n",
              "  (backbone): Sequential(\n",
              "    (0): Conv1d(1, 64, kernel_size=(3,), stride=(1,), padding=(1,))\n",
              "    (1): ReLU()\n",
              "    (2): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (3): Conv1d(64, 128, kernel_size=(3,), stride=(1,), padding=(1,))\n",
              "    (4): ReLU()\n",
              "    (5): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  )\n",
              "  (fc): Linear(in_features=256, out_features=128, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "# Save the trained SimCLR model\n",
        "torch.save(model.state_dict(), 'simclr_model.pth')\n",
        "\n",
        "# Define the fall detection model\n",
        "class FallDetectionModel(nn.Module):\n",
        "    def __init__(self, input_size, num_classes):\n",
        "        super(FallDetectionModel, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_size, 64)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.fc2 = nn.Linear(64, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.fc1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "# Create an instance of the fall detection model\n",
        "num_classes = 2  # Specify the number of classes for fall detection\n",
        "fall_model = FallDetectionModel(input_size=128, num_classes=num_classes)\n",
        "\n",
        "# Define the fall detection loss function\n",
        "fall_loss_function = nn.CrossEntropyLoss()\n",
        "\n",
        "# Define the fall detection optimizer\n",
        "fall_optimizer = optim.SGD(fall_model.parameters(), lr=0.001)\n",
        "\n",
        "# Load the saved SimCLR model and extract learned embeddings\n",
        "simclr_model = SimCLRModel(num_steps)\n",
        "simclr_model.load_state_dict(torch.load('simclr_model.pth'))\n",
        "simclr_model.eval()\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P0Sh2pgifAGT"
      },
      "outputs": [],
      "source": [
        "# To be incorporated into the previous load function\n",
        "# Load and preprocess the accelerometer data\n",
        "def load_and_preprocess(zip_path):\n",
        "\n",
        "    # Extract the CSV files from the zip file\n",
        "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "        csv_files = [file for file in zip_ref.namelist() if file.endswith('.csv')]\n",
        "        zip_ref.extractall(members=csv_files)\n",
        "\n",
        "    # Read and concatenate the extracted CSV files into a DataFrame\n",
        "    data = pd.concat([pd.read_csv(file, encoding='latin-1') for file in csv_files], ignore_index=True)\n",
        "    accelerometer_data = data[[\"Acc(X)\", \"Acc(Y)\", \"Acc(Z)\", \"Rot(X)\", \"Rot(Y)\", \"Rot(Z)\", \"Pitch\", \"Roll\", \"Yaw\", \"Timestamp\"]].values\n",
        "    labels = list(data[\"Fall\"])#.values\n",
        "    # Standardize the data\n",
        "    scaler = StandardScaler()\n",
        "    standardized_data = scaler.fit_transform(accelerometer_data)\n",
        "\n",
        "    return standardized_data,labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FTH3RGMR9JkR"
      },
      "outputs": [],
      "source": [
        "class AccelerometerDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, data, labels):\n",
        "        self.data = data\n",
        "        self.labels = labels\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        sample = self.data[index]\n",
        "        augmented_sample_1 = augment_function(sample)\n",
        "        augmented_sample_2 = augment_function(sample)\n",
        "        label = self.labels[index]  # Get the label for this sample\n",
        "        return augmented_sample_1, augmented_sample_2, label  # Return augmented samples and label\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "34aSnUkgeLyu"
      },
      "outputs": [],
      "source": [
        "\n",
        "# # Load the fall-dataset-features csv files\n",
        "# feature_files = glob.glob('fall-dataset-features/*.csv')\n",
        "# features = []\n",
        "# for feature_file in feature_files:\n",
        "#     df = pd.read_csv(feature_file)\n",
        "#     features.append(df.to_numpy())\n",
        "\n",
        "# # Split the features into train and test sets\n",
        "# train_features, test_features = np.split(features, [int(0.8 * len(features))])\n",
        "\n",
        "# Load the fall-dataset-raw csv files\n",
        "raw_files = \"fall-dataset-raw.zip\"#glob.glob('fall-dataset-raw/*.csv')\n",
        "raw_data, labels = load_and_preprocess(raw_files)\n",
        "\n",
        "raw_data = raw_data[:10000] #1\n",
        "labels = labels[:10000]\n",
        "\n",
        "train_labels=[]\n",
        "test_labels=[]\n",
        "\n",
        "# Split the raw data into train and test sets\n",
        "train_raw_data, test_raw_data = raw_data[:int(0.8 * len(raw_data))], raw_data[int(0.8 * len(raw_data)):]\n",
        "\n",
        "# Split the raw data labels into train and test sets\n",
        "train_labels, test_labels = labels[:int(0.8 * len(raw_data))], labels[int(0.8 * len(raw_data)):]\n",
        "\n",
        "# Define the train and test datasets\n",
        "train_dataset = AccelerometerDataset(train_raw_data,train_labels)\n",
        "test_dataset = AccelerometerDataset(test_raw_data,test_labels)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jG6zPb7HIXD1",
        "outputId": "0c5f1eeb-a5bf-4223-f5a8-6e4328000ef4"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "8000"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "len(train_dataset.data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WKsgovhlMox-"
      },
      "outputs": [],
      "source": [
        "# Define the batch size\n",
        "batch_size = 64\n",
        "# batch_size = 10 #2\n",
        "# Create the data loaders\n",
        "train_data_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_data_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u0TqU7V9TsI3"
      },
      "outputs": [],
      "source": [
        "# batch_size = 10 #2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8kIydI1Pn3ev",
        "outputId": "cf65da40-70fc-48f0-e99b-2c8f00a3fe79"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "SimCLRModel(\n",
              "  (backbone): Sequential(\n",
              "    (0): Conv1d(1, 64, kernel_size=(3,), stride=(1,), padding=(1,))\n",
              "    (1): ReLU()\n",
              "    (2): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (3): Conv1d(64, 128, kernel_size=(3,), stride=(1,), padding=(1,))\n",
              "    (4): ReLU()\n",
              "    (5): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  )\n",
              "  (fc): Linear(in_features=256, out_features=128, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "simclr_model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4kyMqa-PPLOy",
        "outputId": "42f4ae42-0a6c-4add-c3f3-cc11f0384b89"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "8000"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "len(train_dataset.data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fcBRs5sSSgyW",
        "outputId": "c8f564b7-f927-4232-c414-0c9f0c22cb20"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "8000"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "len(train_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vzvhFoJHKpRo"
      },
      "outputs": [],
      "source": [
        "train_embeddings = []\n",
        "train_labels_list = []  # Create a list to hold the labels for each batch\n",
        "\n",
        "for batch in train_data_loader:\n",
        "    # print(batch)\n",
        "    # print('1')\n",
        "    augmented_samples_1, augmented_samples_2, labels = batch\n",
        "    # print(augmented_samples_1)\n",
        "    # print('2')\n",
        "    # print(augmented_samples_2)\n",
        "    # print('3')\n",
        "    augmented_samples_1 = augmented_samples_1.unsqueeze(1).to(device)\n",
        "    augmented_samples_2 = augmented_samples_2.unsqueeze(1).to(device)\n",
        "    # print(augmented_samples_1)\n",
        "    # print('4')\n",
        "    # print(augmented_samples_2)\n",
        "    # print('5')\n",
        "    embeddings_1 = simclr_model(augmented_samples_1.to(device).float())\n",
        "    embeddings_2 = simclr_model(augmented_samples_2.to(device).float())\n",
        "    # print(embeddings_1)\n",
        "    # print('6')\n",
        "    # print(embeddings_2)\n",
        "    # print('7')\n",
        "    # print(len(embeddings_1))\n",
        "    # print(len(embeddings_2))\n",
        "    train_embeddings.append(embeddings_1)\n",
        "    # print(len(train_embeddings))\n",
        "    train_embeddings.append(embeddings_2)\n",
        "    # print(len(train_embeddings))\n",
        "    # print(train_embeddings)\n",
        "    # print('9')\n",
        "\n",
        "    # Create a tensor of batch_size with corresponding labels (assuming train_labels is already a tensor)\n",
        "    # temp_labels = train_labels[len(train_labels_list) : len(train_labels_list) + len(embeddings_1)]\n",
        "    # print(len(temp_labels))\n",
        "\n",
        "    # train_labels_list.extend(temp_labels)\n",
        "    # train_labels_list.extend(temp_labels)\n",
        "    # print(len(train_labels_list))\n",
        "\n",
        "    train_labels_list.extend(labels)\n",
        "    train_labels_list.extend(labels)\n",
        "\n",
        "\n",
        "    # train_labels_list.extend(labels)\n",
        "\n",
        "train_embeddings = torch.cat(train_embeddings, dim=0)\n",
        "# print(len(train_labels_list))\n",
        "train_labels = np.array(train_labels_list)\n",
        "# Concatenate all the elements in train_labels_list to obtain the final train_labels tensor\n",
        "# train_labels = torch.cat(train_labels_list, dim=0)\n",
        "\n",
        "# Similar changes for test_data_loader and test_labels\n",
        "\n",
        "\n",
        "test_embeddings = []\n",
        "test_labels_list = []  # New list to store labels\n",
        "for batch in test_data_loader:\n",
        "    augmented_samples_1, augmented_samples_2, t_labels = batch\n",
        "    augmented_samples_1 = augmented_samples_1.unsqueeze(1).to(device)\n",
        "    augmented_samples_2 = augmented_samples_2.unsqueeze(1).to(device)\n",
        "    embeddings_1 = simclr_model(augmented_samples_1.to(device).float())\n",
        "    embeddings_2 = simclr_model(augmented_samples_2.to(device).float())\n",
        "    test_embeddings.append(embeddings_1)\n",
        "    test_embeddings.append(embeddings_2)\n",
        "\n",
        "    # Get the corresponding labels for this batch\n",
        "    # temp_labels = test_labels[len(test_labels_list) : len(test_labels_list) + len(embeddings_1)]\n",
        "    # print(len(temp_labels))\n",
        "\n",
        "    test_labels_list.extend(t_labels)\n",
        "    test_labels_list.extend(t_labels)\n",
        "\n",
        "# Concatenate the embeddings and labels lists into tensors\n",
        "test_embeddings = torch.cat(test_embeddings, dim=0)\n",
        "test_labels = np.array(test_labels_list)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tOfHZRBEKDMh",
        "outputId": "9c4f6e26-8ce8-4639-f9ee-1d584a47ce72"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "16000"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ],
      "source": [
        "len(train_embeddings)\n",
        "len(train_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yqLJ4OtBoY86"
      },
      "outputs": [],
      "source": [
        "train_labels = torch.tensor(train_labels)\n",
        "test_labels = torch.tensor(test_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D3T4Zti3obLS",
        "outputId": "5a7e2339-5b39-491e-ebda-a5a7d088619e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4000"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ],
      "source": [
        "len(test_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v4nofuK2XurH",
        "outputId": "ade277bf-47cd-4644-ddd9-cc7e5966588c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/10], Fall Detection Loss: 4002.4648423382428, Test Accuracy: 0.5467\n",
            "Epoch [1/10], Fall Detection Loss: 0.25015405264614016\n",
            "Epoch [2/10], Fall Detection Loss: 3180.034875508394, Test Accuracy: 0.5480\n",
            "Epoch [2/10], Fall Detection Loss: 0.19875217971927464\n",
            "Epoch [3/10], Fall Detection Loss: 3109.569534753456, Test Accuracy: 0.5633\n",
            "Epoch [3/10], Fall Detection Loss: 0.194348095922091\n",
            "Epoch [4/10], Fall Detection Loss: 3039.7669946499077, Test Accuracy: 0.5690\n",
            "Epoch [4/10], Fall Detection Loss: 0.18998543716561922\n",
            "Epoch [5/10], Fall Detection Loss: 3000.4687299801517, Test Accuracy: 0.6080\n",
            "Epoch [5/10], Fall Detection Loss: 0.18752929562375947\n",
            "Epoch [6/10], Fall Detection Loss: 2927.3422497412757, Test Accuracy: 0.5470\n",
            "Epoch [6/10], Fall Detection Loss: 0.18295889060882972\n",
            "Epoch [7/10], Fall Detection Loss: 2916.1927434811805, Test Accuracy: 0.5517\n",
            "Epoch [7/10], Fall Detection Loss: 0.18226204646757377\n",
            "Epoch [8/10], Fall Detection Loss: 2822.719527293017, Test Accuracy: 0.5530\n",
            "Epoch [8/10], Fall Detection Loss: 0.17641997045581356\n"
          ]
        }
      ],
      "source": [
        "# Move the fall detection model to the same device as the embeddings\n",
        "fall_model = fall_model.to(device)\n",
        "\n",
        "# Training loop for the fall detection model\n",
        "num_epochs = 10\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    fall_model.train()\n",
        "    total_loss = 0.0  # Track the total loss for the epoch\n",
        "\n",
        "    for i in range(len(train_embeddings)):\n",
        "        # Get the embeddings and labels for the current batch\n",
        "        embeddings = train_embeddings[i].unsqueeze(0)\n",
        "        labels = train_labels[i].unsqueeze(0)\n",
        "\n",
        "        # Move embeddings and labels to the same device as the model\n",
        "        embeddings = embeddings.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        # Convert the inputs and labels to torch.float32\n",
        "        embeddings = embeddings.float()\n",
        "        labels = labels.float()\n",
        "\n",
        "        # Forward pass through the fall detection model\n",
        "        outputs = fall_model(embeddings)\n",
        "\n",
        "        # Calculate the fall detection loss\n",
        "        fall_loss = fall_loss_function(outputs, labels.to(torch.int64))\n",
        "        total_loss += fall_loss.item()\n",
        "\n",
        "        # Zero the gradients\n",
        "        fall_optimizer.zero_grad()\n",
        "\n",
        "        # Backward pass and optimization\n",
        "        fall_loss.backward(retain_graph=True)\n",
        "        fall_optimizer.step()\n",
        "\n",
        "    # Evaluate the fall detection model\n",
        "    fall_model.eval()\n",
        "    with torch.no_grad():\n",
        "        total_correct = 0\n",
        "        total_samples = 0\n",
        "        for i in range(len(test_embeddings)):\n",
        "            # Get the embeddings and labels for the current batch\n",
        "            embeddings = test_embeddings[i].unsqueeze(0)\n",
        "            labels = test_labels[i].unsqueeze(0)\n",
        "\n",
        "            # Move embeddings and labels to the same device as the model\n",
        "            embeddings = embeddings.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            # Convert the inputs and labels to torch.float32\n",
        "            embeddings = embeddings.float()\n",
        "            labels = labels.float()\n",
        "\n",
        "            # Forward pass through the fall detection model\n",
        "            outputs = fall_model(embeddings)\n",
        "\n",
        "            # Calculate accuracy for this batch\n",
        "            predicted = torch.argmax(outputs, 1)\n",
        "            total_correct += (predicted == labels).sum().item()\n",
        "            total_samples += labels.size(0)\n",
        "\n",
        "        accuracy = total_correct / total_samples\n",
        "        print(f\"Epoch [{epoch+1}/{num_epochs}], Fall Detection Loss: {total_loss}, Test Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "    # Calculate the average loss for the epoch\n",
        "    avg_loss = total_loss / len(train_embeddings)\n",
        "\n",
        "    print(f\"Epoch [{epoch+1}/{num_epochs}], Fall Detection Loss: {avg_loss}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NUZIbEsnUb0t"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}